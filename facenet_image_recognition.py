# -*- coding: utf-8 -*-
"""facenet_image_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hs4wqaIHYAsC-ZhmrbbNqlRMBuU4sTwz

# Face Detection
"""

!pip install facenet-pytorch

from facenet_pytorch import MTCNN, InceptionResnetV1

help(MTCNN)

# If required, create a face detection pipeline using MTCNN:
mtcnn = MTCNN()

# Create an inception resnet (in eval mode):
resnet = InceptionResnetV1(pretrained='vggface2').eval()

from PIL import Image

img = Image.open("group_photo.jpg")

# Get cropped and prewhitened image tensor
img_cropped = mtcnn(img, save_path="group_photo_saved.jpg")

# Calculate embedding (unsqueeze to add batch dimension)
img_embedding = resnet(img_cropped.unsqueeze(0))

# Or, if using for VGGFace2 classification
resnet.classify = True
img_probs = resnet(img_cropped.unsqueeze(0))

"""# Custom face comparison using pretrained resnet18"""

import torch
import torchvision.transforms as transforms
from PIL import Image
import torch.nn.functional as F
import torchvision.models as models

class FaceNet(torch.nn.Module):
    def __init__(self):
        super(FaceNet, self).__init__()
        # Define the architecture here (e.g., CNN layers followed by fully connected layers)
        self.cnn_layers = torch.nn.Sequential(
            *list(models.resnet18(pretrained=True).children())[:-2]  # Remove last 2 layers (avgpool and fc)
        )
        # Modify the fully connected layer to output embeddings of desired size
        self.fc = torch.nn.Linear(512*7*7, 128)  # Adjust input size

    def forward(self, x):
        # Forward pass through the CNN layers
        x = self.cnn_layers(x)
        # Flatten the output for the fully connected layer
        x = torch.flatten(x, 1)
        # Forward pass through the fully connected layer
        x = self.fc(x)
        # Apply L2 normalization to the embeddings
        x = F.normalize(x, p=2, dim=1)
        return x

# Step 2: Preprocess the images
def preprocess_image(image_path):
    image = Image.open(image_path).convert('RGB')
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # Resize the image to fit the input size of the model
        transforms.ToTensor(),           # Convert image to tensor
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image
    ])
    image = transform(image)
    return image.unsqueeze(0)  # Add batch dimension

# Step 3: Generate embeddings for the images
def generate_embedding(image_path, model):
    image_tensor = preprocess_image(image_path)
    with torch.no_grad():
        embedding = model(image_tensor)
    return embedding

# Step 4: Calculate similarity between embeddings
def calculate_similarity(embedding1, embedding2):
    # Calculate cosine similarity between the embeddings
    similarity = F.cosine_similarity(embedding1, embedding2)
    return similarity.item()

# Load FaceNet model
face_net_model = FaceNet()

# Load images and generate embeddings
image1_path = "Shah_Rukh_Khan.115.jpg"
image2_path = "Shah_Rukh_Khan.131.jpg"
embedding1 = generate_embedding(image1_path, face_net_model)
embedding2 = generate_embedding(image2_path, face_net_model)

# Calculate similarity between embeddings
similarity_score = calculate_similarity(embedding1, embedding2)
print("Similarity score between the images:", similarity_score)

